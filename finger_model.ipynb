{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce300b88-4d8b-4352-85d8-bec0afebb35d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 훈련 데이터와 테스트 데이터 준비\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m images_paper \u001b[38;5;241m=\u001b[39m \u001b[43mlandmarks_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_landmarks_paper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m images_rock \u001b[38;5;241m=\u001b[39m landmarks_to_image(data_landmarks_rock)\n\u001b[0;32m     52\u001b[0m images_etc \u001b[38;5;241m=\u001b[39m landmarks_to_image(data_landmarks_etc)\n",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m, in \u001b[0;36mlandmarks_to_image\u001b[1;34m(data_landmarks)\u001b[0m\n\u001b[0;32m     40\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 손 모양의 좌표를 이미지에 그리기\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m landmarks:\n\u001b[0;32m     43\u001b[0m     img[\u001b[38;5;28mint\u001b[39m(y \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m), \u001b[38;5;28mint\u001b[39m(x \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 이미지 크기를 28x28로 조정하여 CNN 모델에 맞춤\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 및 결합\n",
    "def load_and_combine_data():\n",
    "    data_landmarks_paper = []\n",
    "    data_landmarks_rock = []\n",
    "    data_landmarks_etc = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(76, 100):\n",
    "        filename_paper = f'landmarks_paper_{i}.csv'\n",
    "        filename_rock = f'landmarks_rock_{i}.csv'\n",
    "        filename_etc = f'landmarks_etc_{i}.csv'\n",
    "\n",
    "        df_paper = pd.read_csv(filename_paper)\n",
    "        df_rock = pd.read_csv(filename_rock)\n",
    "        df_etc = pd.read_csv(filename_etc)\n",
    "\n",
    "        # 가정: 첫 번째부터 네 번째 열까지가 x_r, y_r, x_l, y_l\n",
    "        features_paper = df_paper[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_rock = df_rock[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_etc = df_etc[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "\n",
    "        data_landmarks_paper.append(features_paper)\n",
    "        data_landmarks_rock.append(features_rock)\n",
    "        data_landmarks_etc.append(features_etc)\n",
    "\n",
    "        # 클래스 라벨 부여 (rock, paper, etc)\n",
    "        label = 'paper' if i <= 8 else ('rock' if i <= 16 else 'etc')\n",
    "        labels.append(label)\n",
    "\n",
    "    return data_landmarks_paper, data_landmarks_rock, data_landmarks_etc, labels\n",
    "\n",
    "# 특성 추출 및 데이터 전처리\n",
    "data_landmarks_paper, data_landmarks_rock, data_landmarks_etc, labels = load_and_combine_data()\n",
    "\n",
    "# 이미지로 변환\n",
    "def landmarks_to_image(data_landmarks):\n",
    "    images = []\n",
    "    for landmarks in data_landmarks:\n",
    "        # 0으로 초기화된 100x100 크기의 흑백 이미지 생성\n",
    "        img = np.zeros((100, 100), dtype=np.uint8)\n",
    "        # 손 모양의 좌표를 이미지에 그리기\n",
    "        for x, y in landmarks:\n",
    "            img[int(y * 100), int(x * 100)] = 255\n",
    "        # 이미지 크기를 28x28로 조정하여 CNN 모델에 맞춤\n",
    "        img = cv2.resize(img, (28, 28))\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 준비\n",
    "images_paper = landmarks_to_image(data_landmarks_paper)\n",
    "images_rock = landmarks_to_image(data_landmarks_rock)\n",
    "images_etc = landmarks_to_image(data_landmarks_etc)\n",
    "\n",
    "X = np.array(images_paper + images_rock + images_etc)\n",
    "y = np.array(labels)\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 데이터 차원 추가 (CNN 모델은 입력 데이터로 4D 텐서를 요구합니다)\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# 테스트 데이터로 모델 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29faa2ba-65db-4389-92d4-7200f5148ada",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 훈련 데이터와 테스트 데이터 준비\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m images_paper \u001b[38;5;241m=\u001b[39m \u001b[43mlandmarks_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_landmarks_paper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m images_rock \u001b[38;5;241m=\u001b[39m landmarks_to_image(data_landmarks_rock)\n\u001b[0;32m     66\u001b[0m images_etc \u001b[38;5;241m=\u001b[39m landmarks_to_image(data_landmarks_etc)\n",
      "Cell \u001b[1;32mIn[5], line 56\u001b[0m, in \u001b[0;36mlandmarks_to_image\u001b[1;34m(data_landmarks)\u001b[0m\n\u001b[0;32m     54\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# 손 모양의 좌표를 이미지에 그리기\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m landmarks:\n\u001b[0;32m     57\u001b[0m     img[\u001b[38;5;28mint\u001b[39m(y \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m), \u001b[38;5;28mint\u001b[39m(x \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 이미지 크기를 28x28로 조정하여 CNN 모델에 맞춤\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 데이터 로드 및 결합\n",
    "def load_and_combine_data():\n",
    "    data_landmarks_paper = []\n",
    "    data_landmarks_rock = []\n",
    "    data_landmarks_etc = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(76, 101):\n",
    "        filename_paper = f'landmarks_paper_{i}.csv'\n",
    "        filename_rock = f'landmarks_rock_{i}.csv'\n",
    "        filename_etc = f'landmarks_etc_{i}.csv'\n",
    "\n",
    "        df_paper = pd.read_csv(filename_paper)\n",
    "        df_rock = pd.read_csv(filename_rock)\n",
    "        df_etc = pd.read_csv(filename_etc)\n",
    "\n",
    "        # 가정: 첫 번째부터 네 번째 열까지가 x_r, y_r, x_l, y_l\n",
    "        features_paper = df_paper[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_rock = df_rock[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_etc = df_etc[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "\n",
    "        data_landmarks_paper.append(features_paper)\n",
    "        data_landmarks_rock.append(features_rock)\n",
    "        data_landmarks_etc.append(features_etc)\n",
    "\n",
    "        # 클래스 라벨 부여 (rock, paper, etc)\n",
    "        label_paper = 'paper'\n",
    "        label_rock = 'rock'\n",
    "        label_etc = 'etc'\n",
    "        data_count = len(df_paper)  # 데이터 수는 paper, rock, etc 모두 동일하다고 가정합니다.\n",
    "\n",
    "        labels.extend([label_paper] * data_count)\n",
    "        labels.extend([label_rock] * data_count)\n",
    "        labels.extend([label_etc] * data_count)\n",
    "\n",
    "    return data_landmarks_paper, data_landmarks_rock, data_landmarks_etc, labels\n",
    "\n",
    "\n",
    "# 특성 추출 및 데이터 전처리\n",
    "data_landmarks_paper, data_landmarks_rock, data_landmarks_etc, labels = load_and_combine_data()\n",
    "\n",
    "# 이미지로 변환\n",
    "def landmarks_to_image(data_landmarks):\n",
    "    images = []\n",
    "    for landmarks in data_landmarks:\n",
    "        # 0으로 초기화된 100x100 크기의 흑백 이미지 생성\n",
    "        img = np.zeros((100, 100), dtype=np.uint8)\n",
    "        # 손 모양의 좌표를 이미지에 그리기\n",
    "        for x, y in landmarks:\n",
    "            img[int(y * 100), int(x * 100)] = 255\n",
    "        # 이미지 크기를 28x28로 조정하여 CNN 모델에 맞춤\n",
    "        img = cv2.resize(img, (28, 28))\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 준비\n",
    "images_paper = landmarks_to_image(data_landmarks_paper)\n",
    "images_rock = landmarks_to_image(data_landmarks_rock)\n",
    "images_etc = landmarks_to_image(data_landmarks_etc)\n",
    "\n",
    "X = np.array(images_paper + images_rock + images_etc)\n",
    "y = np.array(labels)\n",
    "\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 데이터 차원 추가 (CNN 모델은 입력 데이터로 4D 텐서를 요구합니다)\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# 테스트 데이터로 모델 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# 분류 보고서 출력\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a2e2ea-25fc-4c0a-a82e-16fd2d5d3e94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "train_size=25 should be either positive and smaller than the number of samples 25 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m images\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# 데이터 로드 및 결합\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m data_landmarks_paper_train, data_landmarks_paper_test, data_landmarks_rock_train, data_landmarks_rock_test, data_landmarks_etc_train, data_landmarks_etc_test, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_combine_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# 훈련 데이터와 테스트 데이터 준비\u001b[39;00m\n\u001b[0;32m     71\u001b[0m images_paper_train \u001b[38;5;241m=\u001b[39m landmarks_to_image(data_landmarks_paper_train)\n",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m, in \u001b[0;36mload_and_combine_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m min_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(counts)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 데이터 분할\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m data_landmarks_paper_train, data_landmarks_paper_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_landmarks_paper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m data_landmarks_rock_train, data_landmarks_rock_test \u001b[38;5;241m=\u001b[39m train_test_split(data_landmarks_rock, train_size\u001b[38;5;241m=\u001b[39mmin_count, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     45\u001b[0m data_landmarks_etc_train, data_landmarks_etc_test \u001b[38;5;241m=\u001b[39m train_test_split(data_landmarks_etc, train_size\u001b[38;5;241m=\u001b[39mmin_count, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2649\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2646\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2648\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2649\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2262\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_size=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m should be either positive and smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m or a float in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0, 1) range\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_size, n_samples)\n\u001b[0;32m   2254\u001b[0m     )\n\u001b[0;32m   2256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2257\u001b[0m     train_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (train_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;129;01mor\u001b[39;00m train_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m train_size_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (train_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m train_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2261\u001b[0m ):\n\u001b[1;32m-> 2262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_size=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m should be either positive and smaller\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m or a float in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(0, 1) range\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_size, n_samples)\n\u001b[0;32m   2266\u001b[0m     )\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m train_size_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid value for train_size: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_size))\n",
      "\u001b[1;31mValueError\u001b[0m: train_size=25 should be either positive and smaller than the number of samples 25 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터 로드 및 결합\n",
    "def load_and_combine_data():\n",
    "    data_landmarks_paper = []\n",
    "    data_landmarks_rock = []\n",
    "    data_landmarks_etc = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(76, 101):\n",
    "        filename_paper = f'landmarks_paper_{i}.csv'\n",
    "        filename_rock = f'landmarks_rock_{i}.csv'\n",
    "        filename_etc = f'landmarks_etc_{i}.csv'\n",
    "\n",
    "        df_paper = pd.read_csv(filename_paper)\n",
    "        df_rock = pd.read_csv(filename_rock)\n",
    "        df_etc = pd.read_csv(filename_etc)\n",
    "\n",
    "        # 가정: 첫 번째부터 네 번째 열까지가 x_r, y_r, x_l, y_l\n",
    "        features_paper = df_paper[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_rock = df_rock[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_etc = df_etc[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "\n",
    "        data_landmarks_paper.append(features_paper)\n",
    "        data_landmarks_rock.append(features_rock)\n",
    "        data_landmarks_etc.append(features_etc)\n",
    "\n",
    "    # 클래스 라벨 부여 (rock, paper, etc)\n",
    "    label_paper = 'paper'\n",
    "    label_rock = 'rock'\n",
    "    label_etc = 'etc'\n",
    "\n",
    "    # 데이터 수 확인\n",
    "    counts = [len(data_landmarks_paper), len(data_landmarks_rock), len(data_landmarks_etc)]\n",
    "    min_count = min(counts)\n",
    "\n",
    "    # 데이터 분할\n",
    "    data_landmarks_paper_train, data_landmarks_paper_test = train_test_split(data_landmarks_paper, train_size=min_count, random_state=42)\n",
    "    data_landmarks_rock_train, data_landmarks_rock_test = train_test_split(data_landmarks_rock, train_size=min_count, random_state=42)\n",
    "    data_landmarks_etc_train, data_landmarks_etc_test = train_test_split(data_landmarks_etc, train_size=min_count, random_state=42)\n",
    "\n",
    "    labels.extend([label_paper] * min_count)\n",
    "    labels.extend([label_rock] * min_count)\n",
    "    labels.extend([label_etc] * min_count)\n",
    "\n",
    "    return data_landmarks_paper_train, data_landmarks_paper_test, data_landmarks_rock_train, data_landmarks_rock_test, data_landmarks_etc_train, data_landmarks_etc_test, labels\n",
    "\n",
    "# 특성 추출 및 데이터 전처리\n",
    "def landmarks_to_image(data_landmarks):\n",
    "    images = []\n",
    "    for landmarks in data_landmarks:\n",
    "        # 0으로 초기화된 100x100 크기의 흑백 이미지 생성\n",
    "        img = np.zeros((100, 100), dtype=np.uint8)\n",
    "        # 손 모양의 좌표를 이미지에 그리기\n",
    "        for x, y in landmarks:\n",
    "            img[int(y * 100), int(x * 100)] = 255\n",
    "        # 이미지 크기를 28x28로 조정하여 CNN 모델에 맞춤\n",
    "        img = cv2.resize(img, (28, 28))\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "# 데이터 로드 및 결합\n",
    "data_landmarks_paper_train, data_landmarks_paper_test, data_landmarks_rock_train, data_landmarks_rock_test, data_landmarks_etc_train, data_landmarks_etc_test, labels = load_and_combine_data()\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 준비\n",
    "images_paper_train = landmarks_to_image(data_landmarks_paper_train)\n",
    "images_paper_test = landmarks_to_image(data_landmarks_paper_test)\n",
    "images_rock_train = landmarks_to_image(data_landmarks_rock_train)\n",
    "images_rock_test = landmarks_to_image(data_landmarks_rock_test)\n",
    "images_etc_train = landmarks_to_image(data_landmarks_etc_train)\n",
    "images_etc_test = landmarks_to_image(data_landmarks_etc_test)\n",
    "\n",
    "X_train = np.array(images_paper_train + images_rock_train + images_etc_train)\n",
    "X_test = np.array(images_paper_test + images_rock_test + images_etc_test)\n",
    "y_train = np.array(labels * 2)  # train 데이터는 각 클래스별로 같은 개수로 복사되었으므로 2배\n",
    "y_test = np.array(labels)  # test 데이터는 원래 클래스 순서 그대로\n",
    "\n",
    "# CNN 모델 생성\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 데이터 차원 추가 (CNN 모델은 입력 데이터로 4D 텐서를 요구합니다)\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# 테스트 데이터로 모델 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# 분류 보고서 출력\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d5adbc6-ecb7-4bba-aba3-03f6e788ed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.92307692 1.         1.         0.91666667 1.        ]\n",
      "Mean Accuracy: 0.9679487179487178\n",
      "Confusion Matrix:\n",
      "[[5 0 1]\n",
      " [0 6 0]\n",
      " [0 0 1]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.83      0.91         6\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.92        13\n",
      "   macro avg       0.83      0.94      0.86        13\n",
      "weighted avg       0.96      0.92      0.93        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 로드 및 결합\n",
    "def load_and_combine_data():\n",
    "    data_landmarks_paper = []\n",
    "    data_landmarks_rock = []\n",
    "    data_landmarks_etc = []\n",
    "\n",
    "    for i in range(76, 100):  # Assuming the file naming convention follows this pattern\n",
    "        filename_paper = f'landmarks_paper_{i}.csv'\n",
    "        filename_rock = f'landmarks_rock_{i}.csv'\n",
    "        filename_etc = f'landmarks_etc_{i}.csv'\n",
    "\n",
    "        df_paper = pd.read_csv(filename_paper)\n",
    "        df_rock = pd.read_csv(filename_rock)\n",
    "        df_etc = pd.read_csv(filename_etc)\n",
    "\n",
    "        # 가정: 첫 번째부터 네 번째 열까지가 x_r, y_r, x_l, y_l\n",
    "        features_paper = df_paper[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_rock = df_rock[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_etc = df_etc[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "\n",
    "        # Check if data is not empty\n",
    "        if len(features_paper) > 0 and len(features_rock) > 0 and len(features_etc) > 0:\n",
    "            # Standardize each feature separately\n",
    "            scaler = StandardScaler()\n",
    "            features_paper = scaler.fit_transform(features_paper)\n",
    "            features_rock = scaler.fit_transform(features_rock)\n",
    "            features_etc = scaler.fit_transform(features_etc)\n",
    "\n",
    "            data_landmarks_paper.append(features_paper)\n",
    "            data_landmarks_rock.append(features_rock)\n",
    "            data_landmarks_etc.append(features_etc)\n",
    "\n",
    "    return data_landmarks_paper, data_landmarks_rock, data_landmarks_etc\n",
    "\n",
    "# 특성 추출 및 데이터 전처리\n",
    "data_landmarks_paper, data_landmarks_rock, data_landmarks_etc = load_and_combine_data()\n",
    "\n",
    "# Combine standardized features\n",
    "data_combined = []\n",
    "labels_combined = []\n",
    "\n",
    "min_samples_paper = len(data_landmarks_paper)\n",
    "min_samples_rock = len(data_landmarks_rock)\n",
    "min_samples_etc = len(data_landmarks_etc)\n",
    "\n",
    "min_samples = min(min_samples_paper, min_samples_rock, min_samples_etc)\n",
    "\n",
    "for i in range(min_samples):\n",
    "    features_paper = data_landmarks_paper[i].flatten()  # Flatten each sample individually\n",
    "    features_rock = data_landmarks_rock[i].flatten()  # Flatten each sample individually\n",
    "    features_etc = data_landmarks_etc[i].flatten()  # Flatten each sample individually\n",
    "\n",
    "    # Check if data is not empty\n",
    "    if len(features_paper) > 0 and len(features_rock) > 0 and len(features_etc) > 0:\n",
    "        data_combined.append(list(features_paper))\n",
    "        data_combined.append(list(features_rock))\n",
    "        data_combined.append(list(features_etc))\n",
    "\n",
    "        # Add labels for each sample individually\n",
    "        labels_combined.append(0)  # Label for data from landmarks_paper\n",
    "        labels_combined.append(1)  # Label for data from landmarks_rock\n",
    "        labels_combined.append(2)  # Label for data from landmarks_etc\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_combined, labels_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\n",
    "cv_scores = cross_val_score(model, data_combined, labels_combined, cv=5)  # Use 5-fold cross-validation\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean Accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# 분류 보고서 출력\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0798d9e9-a546-4cce-8f8e-949f90e7e41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 2s 120ms/step - loss: 15.8874 - accuracy: 0.6010 - val_loss: 2.8679 - val_accuracy: 0.7917\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.9097 - accuracy: 0.8702 - val_loss: 2.3799 - val_accuracy: 0.8333\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1335 - accuracy: 0.9519 - val_loss: 1.5049 - val_accuracy: 0.7083\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0286 - accuracy: 0.9952 - val_loss: 1.0500 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.8168 - val_accuracy: 0.7917\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.7298 - val_accuracy: 0.7917\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.7016 - val_accuracy: 0.7917\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.6995 - val_accuracy: 0.7917\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7039 - val_accuracy: 0.7917\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.7108 - val_accuracy: 0.7917\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 1.2718 - accuracy: 0.7797\n",
      "Test accuracy: 0.7796609997749329\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "# 데이터 로드 및 결합\n",
    "def load_and_combine_data():\n",
    "    data_landmarks_paper = []\n",
    "    data_landmarks_rock = []\n",
    "    data_landmarks_etc = []\n",
    "\n",
    "    for i in range(301, 400):  # Assuming the file naming convention follows this pattern'\n",
    "        filename_0 = f'landmarks_0/landmarks_0_{i}.csv'\n",
    "        filename_1 = f'landmarks_1/landmarks_1_{i}.csv'\n",
    "        filename_2 = f'landmakrs_2/landmarks_2_{i}.csv'\n",
    "    \n",
    "        df_paper = pd.read_csv(filename_0)\n",
    "        df_rock = pd.read_csv(filename_1)\n",
    "        df_etc = pd.read_csv(filename_2)\n",
    "\n",
    "        # 가정: 첫 번째부터 네 번째 열까지가 x_r, y_r, x_l, y_l\n",
    "        features_paper = df_paper[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_rock = df_rock[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_etc = df_etc[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "\n",
    "        # Check if data is not empty\n",
    "        if len(features_paper) > 0 and len(features_rock) > 0 and len(features_etc) > 0:\n",
    "            # Standardize each feature separately\n",
    "            scaler = StandardScaler()\n",
    "            features_paper = scaler.fit_transform(features_paper)\n",
    "            features_rock = scaler.fit_transform(features_rock)\n",
    "            features_etc = scaler.fit_transform(features_etc)\n",
    "\n",
    "            data_landmarks_paper.append(features_paper)\n",
    "            data_landmarks_rock.append(features_rock)\n",
    "            data_landmarks_etc.append(features_etc)\n",
    "\n",
    "    return data_landmarks_paper, data_landmarks_rock, data_landmarks_etc\n",
    "\n",
    "# 데이터 이미지화\n",
    "def convert_to_images(data):\n",
    "    images = []\n",
    "    for sample in data:\n",
    "        # Create a blank image\n",
    "        img = np.zeros((100, 100), dtype=np.uint8)\n",
    "        for row in sample:\n",
    "            x_left, y_left, x_right, y_right = row  # Extract coordinates for each hand\n",
    "            x_left_scaled, y_left_scaled = int(x_left * 50 + 50), int(y_left * 50 + 50)  # Scale and shift coordinates for left hand\n",
    "            x_right_scaled, y_right_scaled = int(x_right * 50 + 50), int(y_right * 50 + 50)  # Scale and shift coordinates for right hand\n",
    "            cv2.circle(img, (x_left_scaled, y_left_scaled), 2, (255, 255, 255), -1)  # Draw a white circle for left hand landmark\n",
    "            cv2.circle(img, (x_right_scaled, y_right_scaled), 2, (255, 255, 255), -1)  # Draw a white circle for right hand landmark\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# 특성 추출 및 데이터 전처리\n",
    "data_landmarks_paper, data_landmarks_rock, data_landmarks_etc = load_and_combine_data()\n",
    "\n",
    "# Convert landmarks data to images\n",
    "images_paper = convert_to_images(data_landmarks_paper)\n",
    "images_rock = convert_to_images(data_landmarks_rock)\n",
    "images_etc = convert_to_images(data_landmarks_etc)\n",
    "\n",
    "# Combine images and labels\n",
    "X = np.concatenate((images_paper, images_rock, images_etc), axis=0)\n",
    "y = np.array([0] * len(images_paper) + [1] * len(images_rock) + [2] * len(images_etc))\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN 모델 생성\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape((100, 100, 1), input_shape=(100, 100)),  # Reshape input for CNN\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')  # Output layer with 3 classes\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# 모델 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25fae71-4bea-4da7-820d-ec2532949d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'flatten' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m     11\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,)) \u001b[38;5;66;03m# shape는 데이터 개수를 의미하는 맨 앞의 축을 제외하고 입력\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mflatten\u001b[49m(inputs)\n\u001b[0;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m dense1(x)\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m dense2(x)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'flatten' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape=(4,)) # shape는 데이터 개수를 의미하는 맨 앞의 축을 제외하고 입력\n",
    "x = flatten(inputs)\n",
    "x = dense1(x)\n",
    "x = dense2(x)\n",
    "x = dense3(x)\n",
    "x = dense_out(x)\n",
    "out = softmax(x)\n",
    "\n",
    "# 데이터 로드 및 결합\n",
    "def load_and_combine_data():\n",
    "    data_landmarks_paper = []\n",
    "    data_landmarks_rock = []\n",
    "    data_landmarks_etc = []\n",
    "\n",
    "    for i in range(301, 400):  # Assuming the file naming convention follows this pattern\n",
    "        filename_0 = f'landmarks_0/landmarks_0_{i}.csv'\n",
    "        filename_1 = f'landmarks_1/landmarks_1_{i}.csv'\n",
    "        filename_2 = f'landmakrs_2/landmarks_2_{i}.csv'\n",
    "    \n",
    "        df_paper = pd.read_csv(filename_0)\n",
    "        df_rock = pd.read_csv(filename_1)\n",
    "        df_etc = pd.read_csv(filename_2)\n",
    "\n",
    "        # 가정: 첫 번째부터 네 번째 열까지가 x_r, y_r, x_l, y_l\n",
    "        features_paper = df_paper[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_rock = df_rock[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_etc = df_etc[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "\n",
    "        # Check if data is not empty\n",
    "        if len(features_paper) > 0 and len(features_rock) > 0 and len(features_etc) > 0:\n",
    "            # Standardize each feature separately\n",
    "            scaler = StandardScaler()\n",
    "            features_paper = scaler.fit_transform(features_paper)\n",
    "            features_rock = scaler.fit_transform(features_rock)\n",
    "            features_etc = scaler.fit_transform(features_etc)\n",
    "\n",
    "            data_landmarks_paper.append(features_paper)\n",
    "            data_landmarks_rock.append(features_rock)\n",
    "            data_landmarks_etc.append(features_etc)\n",
    "\n",
    "    return data_landmarks_paper, data_landmarks_rock, data_landmarks_etc\n",
    "\n",
    "# 데이터 특성 추출 및 전처리\n",
    "data_landmarks_paper, data_landmarks_rock, data_landmarks_etc = load_and_combine_data()\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_paper = np.array(data_landmarks_paper)\n",
    "X_rock = np.array(data_landmarks_rock)\n",
    "X_etc = np.array(data_landmarks_etc)\n",
    "\n",
    "# 결합된 데이터 생성\n",
    "X = np.vstack((X_paper, X_rock, X_etc))\n",
    "y = np.array([0] * len(X_paper) + [1] * len(X_rock) + [2] * len(X_etc))\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# DNN 모델 생성\n",
    "flatten = layers.Flatten()\n",
    "dense1 = layers.Dense(64, activation=\"relu\")\n",
    "dense2 = layers.Dense(32, activation=\"relu\")\n",
    "dense3 = layers.Dense(16, activation=\"relu\")\n",
    "dense_out = layers.Dense(4)\n",
    "softmax = layers.Softmax()\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape=(4,)) # shape는 데이터 개수를 의미하는 맨 앞의 축을 제외하고 입력\n",
    "x = flatten(inputs)\n",
    "x = dense1(x)\n",
    "x = dense2(x)\n",
    "x = dense3(x)\n",
    "x = dense_out(x)\n",
    "out = softmax(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=dense_out)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# 모델 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# 모델 요약\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e04d2dd-63bd-4185-9fc7-95fc53372b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "18/18 [==============================] - 4s 48ms/step - loss: 0.9430 - accuracy: 0.6046 - val_loss: 0.6671 - val_accuracy: 0.7656\n",
      "Epoch 2/10\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.5578 - accuracy: 0.8401 - val_loss: 0.4520 - val_accuracy: 0.8906\n",
      "Epoch 3/10\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.3709 - accuracy: 0.9069 - val_loss: 0.3200 - val_accuracy: 0.9219\n",
      "Epoch 4/10\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.2562 - accuracy: 0.9367 - val_loss: 0.2423 - val_accuracy: 0.9375\n",
      "Epoch 5/10\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.1838 - accuracy: 0.9578 - val_loss: 0.2093 - val_accuracy: 0.9219\n",
      "Epoch 6/10\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1470 - accuracy: 0.9649 - val_loss: 0.1573 - val_accuracy: 0.9531\n",
      "Epoch 7/10\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.1197 - accuracy: 0.9684 - val_loss: 0.1572 - val_accuracy: 0.9375\n",
      "Epoch 8/10\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0931 - accuracy: 0.9824 - val_loss: 0.1347 - val_accuracy: 0.9844\n",
      "Epoch 9/10\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 0.0834 - accuracy: 0.9807 - val_loss: 0.1280 - val_accuracy: 0.9375\n",
      "Epoch 10/10\n",
      "18/18 [==============================] - 0s 9ms/step - loss: 0.0703 - accuracy: 0.9895 - val_loss: 0.1190 - val_accuracy: 0.9531\n",
      "5/5 [==============================] - 1s 6ms/step - loss: 0.1387 - accuracy: 0.9497\n",
      "Test accuracy: 0.9496855139732361\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 21, 4)]           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 84)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                5440      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 4)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8116 (31.70 KB)\n",
      "Trainable params: 8116 (31.70 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#DNN \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 데이터 로드 및 결합\n",
    "def load_and_combine_data():\n",
    "    data_landmarks_paper = []\n",
    "    data_landmarks_rock = []\n",
    "    data_landmarks_etc = []\n",
    "\n",
    "    for i in range(0, 400):  # Assuming the file naming convention follows this pattern\n",
    "        filename_0 = f'landmarks_0/landmarks_0_{i}.csv'\n",
    "        filename_1 = f'landmarks_1/landmarks_1_{i}.csv'\n",
    "        filename_2 = f'landmakrs_2/landmarks_2_{i}.csv'\n",
    "\n",
    "        # 파일이 존재하는지 확인하고 없으면 다음 번호로 넘어감\n",
    "        if not (os.path.isfile(filename_0) and os.path.isfile(filename_1) and os.path.isfile(filename_2)):\n",
    "            continue \n",
    "\n",
    "          # 파일이 비어 있는지 확인하고 비어 있으면 다음 번호로 넘어감\n",
    "        if os.path.getsize(filename_0) == 0 or os.path.getsize(filename_1) == 0 or os.path.getsize(filename_2) == 0:\n",
    "            continue\n",
    "            \n",
    "        df_paper = pd.read_csv(filename_0)\n",
    "        df_rock = pd.read_csv(filename_1)\n",
    "        df_etc = pd.read_csv(filename_2)\n",
    "\n",
    "        # 가정: 첫 번째부터 네 번째 열까지가 x_r, y_r, x_l, y_l\n",
    "        features_paper = df_paper[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_rock = df_rock[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_etc = df_etc[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "\n",
    "        # Check if data is not empty\n",
    "        if len(features_paper) > 0 and len(features_rock) > 0 and len(features_etc) > 0:\n",
    "            # Standardize each feature separately\n",
    "            scaler = StandardScaler()\n",
    "            features_paper = scaler.fit_transform(features_paper)\n",
    "            features_rock = scaler.fit_transform(features_rock)\n",
    "            features_etc = scaler.fit_transform(features_etc)\n",
    "\n",
    "            data_landmarks_paper.append(features_paper)\n",
    "            data_landmarks_rock.append(features_rock)\n",
    "            data_landmarks_etc.append(features_etc)\n",
    "\n",
    "    return data_landmarks_paper, data_landmarks_rock, data_landmarks_etc\n",
    "\n",
    "# 데이터 특성 추출 및 전처리\n",
    "data_landmarks_paper, data_landmarks_rock, data_landmarks_etc = load_and_combine_data()\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_paper = np.array(data_landmarks_paper)\n",
    "X_rock = np.array(data_landmarks_rock)\n",
    "X_etc = np.array(data_landmarks_etc)\n",
    "\n",
    "# 결합된 데이터 생성\n",
    "X = np.vstack((X_paper, X_rock, X_etc))\n",
    "y = np.array([0] * len(X_paper) + [1] * len(X_rock) + [2] * len(X_etc))\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# DNN 모델 생성\n",
    "inputs = tf.keras.Input(shape=(21, 4)) # shape는 데이터 개수를 의미하는 맨 앞의 축을 제외하고 입력\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "x = layers.Dense(4)(x)\n",
    "out = layers.Softmax()(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# 모델 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# 모델 요약\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41da9464-0424-4cc2-a344-9aef1247743c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hand_gesture_model.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# 모델 저장\n",
    "joblib.dump(model, 'hand_gesture_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b4d667-d942-4423-aaef-9a585926df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: hand_gesture_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: hand_gesture_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow SavedModel 형식으로 모델 저장\n",
    "model.save(\"hand_gesture_model\", save_format='tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a095c8-db5d-42ab-b939-a0cf50fbe69a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['x_r', 'y_r', 'x_l', 'y_l'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_landmarks_paper, data_landmarks_rock, data_landmarks_etc\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 데이터 특성 추출 및 전처리\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m data_landmarks_paper, data_landmarks_rock, data_landmarks_etc \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_combine_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# 데이터셋 분할\u001b[39;00m\n\u001b[0;32m     60\u001b[0m X_paper \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data_landmarks_paper)\n",
      "Cell \u001b[1;32mIn[2], line 38\u001b[0m, in \u001b[0;36mload_and_combine_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m df_etc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(filename_0)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# 가정: 첫 번째부터 네 번째 열까지가 x_r, y_r, x_l, y_l\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m features_paper \u001b[38;5;241m=\u001b[39m \u001b[43mdf_paper\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_r\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_r\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_l\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_l\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     39\u001b[0m features_rock \u001b[38;5;241m=\u001b[39m df_rock[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_r\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_r\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_l\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_l\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     40\u001b[0m features_etc \u001b[38;5;241m=\u001b[39m df_etc[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_r\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_r\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_l\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_l\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['x_r', 'y_r', 'x_l', 'y_l'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "#DNN \n",
    "#사각형 내 좌표 학습 및 모델 저장 \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 데이터 로드 및 결합\n",
    "def load_and_combine_data():\n",
    "    data_landmarks_paper = []\n",
    "    data_landmarks_rock = []\n",
    "    data_landmarks_etc = []\n",
    "\n",
    "    for i in range(301, 400):  # Assuming the file naming convention follows this pattern\n",
    "        filename_0 = f'landmarks_00/landmarks_0_{i}.csv'\n",
    "        filename_1 = f'landmarks_11/landmarks_1_{i}.csv'\n",
    "        filename_2 = f'landmarks_22/landmarks_2_{i}.csv'\n",
    "\n",
    "        # 파일이 존재하는지 확인하고 없으면 다음 번호로 넘어감\n",
    "        if not (os.path.isfile(filename_0) and os.path.isfile(filename_1) and os.path.isfile(filename_2)):\n",
    "            continue \n",
    "\n",
    "          # 파일이 비어 있는지 확인하고 비어 있으면 다음 번호로 넘어감\n",
    "        if os.path.getsize(filename_0) == 0 or os.path.getsize(filename_1) == 0 or os.path.getsize(filename_2) == 0:\n",
    "            continue\n",
    "            \n",
    "        df_paper = pd.read_csv(filename_2)\n",
    "        df_rock = pd.read_csv(filename_1)\n",
    "        df_etc = pd.read_csv(filename_0)\n",
    "\n",
    "        # 가정: 첫 번째부터 네 번째 열까지가 x_r, y_r, x_l, y_l\n",
    "        features_paper = df_paper[['x_r_0', 'y_r_0', 'x_l', 'y_l']].values\n",
    "        features_rock = df_rock[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "        features_etc = df_etc[['x_r', 'y_r', 'x_l', 'y_l']].values\n",
    "\n",
    "        # Check if data is not empty\n",
    "        if len(features_paper) > 0 and len(features_rock) > 0 and len(features_etc) > 0:\n",
    "            # Standardize each feature separately\n",
    "            scaler = StandardScaler()\n",
    "            features_paper = scaler.fit_transform(features_paper)\n",
    "            features_rock = scaler.fit_transform(features_rock)\n",
    "            features_etc = scaler.fit_transform(features_etc)\n",
    "\n",
    "            data_landmarks_paper.append(features_paper)\n",
    "            data_landmarks_rock.append(features_rock)\n",
    "            data_landmarks_etc.append(features_etc)\n",
    "\n",
    "    return data_landmarks_paper, data_landmarks_rock, data_landmarks_etc\n",
    "\n",
    "# 데이터 특성 추출 및 전처리\n",
    "data_landmarks_paper, data_landmarks_rock, data_landmarks_etc = load_and_combine_data()\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_paper = np.array(data_landmarks_paper)\n",
    "X_rock = np.array(data_landmarks_rock)\n",
    "X_etc = np.array(data_landmarks_etc)\n",
    "\n",
    "# 결합된 데이터 생성\n",
    "X = np.vstack((X_paper, X_rock, X_etc))\n",
    "y = np.array([0] * len(X_paper) + [1] * len(X_rock) + [2] * len(X_etc))\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 형태 변경\n",
    "X_train_reshaped = X_train.reshape(-1, 21, 1, 4)\n",
    "X_test_reshaped = X_test.reshape(-1, 21, 1, 4)\n",
    "\n",
    "# DNN 모델 생성\n",
    "inputs = tf.keras.Input(shape=(84)) # shape는 데이터 개수를 의미하는 맨 앞의 축을 제외하고 입력\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "x = layers.Dense(4)(x)\n",
    "out = layers.Softmax()(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# 모델 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "# 모델 요약\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af7cb6e-a621-4bbd-9193-9749dba878bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "51/51 [==============================] - 1s 6ms/step - loss: 0.8076 - accuracy: 0.7489 - val_loss: 0.2320 - val_accuracy: 0.9611\n",
      "Epoch 2/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.1736 - accuracy: 0.9536 - val_loss: 0.0694 - val_accuracy: 0.9944\n",
      "Epoch 3/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0802 - accuracy: 0.9876 - val_loss: 0.0309 - val_accuracy: 0.9944\n",
      "Epoch 4/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0524 - accuracy: 0.9920 - val_loss: 0.0288 - val_accuracy: 0.9944\n",
      "Epoch 5/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0410 - accuracy: 0.9926 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0328 - accuracy: 0.9932 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 0.9932 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.9932 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.9938 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.9951 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 1/10\n",
      "51/51 [==============================] - 1s 5ms/step - loss: 1.0767 - accuracy: 0.5108 - val_loss: 0.5900 - val_accuracy: 0.8444\n",
      "Epoch 2/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.9307 - val_loss: 0.1444 - val_accuracy: 0.9778\n",
      "Epoch 3/10\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 0.1173 - accuracy: 0.9716 - val_loss: 0.0628 - val_accuracy: 0.9778\n",
      "Epoch 4/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0726 - accuracy: 0.9858 - val_loss: 0.0483 - val_accuracy: 0.9778\n",
      "Epoch 5/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0536 - accuracy: 0.9907 - val_loss: 0.0272 - val_accuracy: 0.9889\n",
      "Epoch 6/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9907 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 0.9901 - val_loss: 0.0176 - val_accuracy: 0.9944\n",
      "Epoch 8/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0367 - accuracy: 0.9907 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0339 - accuracy: 0.9932 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9913 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9933\n",
      "15/15 [==============================] - 0s 349us/step - loss: 0.0476 - accuracy: 0.9956\n",
      "Left Hand Test accuracy: 0.9933333396911621\n",
      "Right Hand Test accuracy: 0.995555579662323\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 21, 2)]           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 42)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2752      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 4)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5428 (21.20 KB)\n",
      "Trainable params: 5428 (21.20 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 21, 2)]           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 42)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                2752      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      " softmax_1 (Softmax)         (None, 4)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5428 (21.20 KB)\n",
      "Trainable params: 5428 (21.20 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 데이터 로드 및 결합\n",
    "def load_and_combine_data():\n",
    "    data_left_hand_paper = []\n",
    "    data_right_hand_paper = []\n",
    "    data_left_hand_rock = []\n",
    "    data_right_hand_rock = []\n",
    "    data_left_hand_etc = []\n",
    "    data_right_hand_etc = []\n",
    "\n",
    "    for i in range(0, 990):  # Assuming the file naming convention follows this pattern\n",
    "        filename_left_paper = f'landmarks_000/left_hand_landmarks_0_{i}.csv'\n",
    "        filename_right_paper = f'landmarks_000/right_hand_landmarks_0_{i}.csv'\n",
    "        filename_left_rock = f'landmarks_111/left_hand_landmarks_1_{i}.csv'\n",
    "        filename_right_rock = f'landmarks_111/right_hand_landmarks_1_{i}.csv'\n",
    "        filename_left_etc = f'landmarks_222/left_hand_landmarks_2_{i}.csv'\n",
    "        filename_right_etc = f'landmarks_222/right_hand_landmarks_2_{i}.csv'\n",
    "\n",
    "        # 파일이 존재하는지 확인하고 없으면 다음 번호로 넘어감\n",
    "        if not (os.path.isfile(filename_left_paper) and os.path.isfile(filename_right_paper) and\n",
    "                os.path.isfile(filename_left_rock) and os.path.isfile(filename_right_rock) and\n",
    "                os.path.isfile(filename_left_etc) and os.path.isfile(filename_right_etc)):\n",
    "            continue \n",
    "\n",
    "        # 파일이 비어 있는지 확인하고 비어 있으면 다음 번호로 넘어감\n",
    "        if (os.path.getsize(filename_left_paper) == 0 or os.path.getsize(filename_right_paper) == 0 or\n",
    "                os.path.getsize(filename_left_rock) == 0 or os.path.getsize(filename_right_rock) == 0 or\n",
    "                os.path.getsize(filename_left_etc) == 0 or os.path.getsize(filename_right_etc) == 0):\n",
    "            continue\n",
    "            \n",
    "        df_left_paper = pd.read_csv(filename_left_paper)\n",
    "        df_right_paper = pd.read_csv(filename_right_paper)\n",
    "        df_left_rock = pd.read_csv(filename_left_rock)\n",
    "        df_right_rock = pd.read_csv(filename_right_rock)\n",
    "        df_left_etc = pd.read_csv(filename_left_etc)\n",
    "        df_right_etc = pd.read_csv(filename_right_etc)\n",
    "\n",
    "        # 가정: 각 파일에서 첫 21개 행이 한 랜드마크의 x, y 좌표\n",
    "        left_hand_landmarks_paper = df_left_paper.values[:21]\n",
    "        right_hand_landmarks_paper = df_right_paper.values[:21]\n",
    "        left_hand_landmarks_rock = df_left_rock.values[:21]\n",
    "        right_hand_landmarks_rock = df_right_rock.values[:21]\n",
    "        left_hand_landmarks_etc = df_left_etc.values[:21]\n",
    "        right_hand_landmarks_etc = df_right_etc.values[:21]\n",
    "\n",
    "        # Check if data is not empty\n",
    "        if (len(left_hand_landmarks_paper) == 21 and len(right_hand_landmarks_paper) == 21 and\n",
    "                len(left_hand_landmarks_rock) == 21 and len(right_hand_landmarks_rock) == 21 and\n",
    "                len(left_hand_landmarks_etc) == 21 and len(right_hand_landmarks_etc) == 21):\n",
    "            # Standardize each feature separately\n",
    "            scaler = StandardScaler()\n",
    "            left_hand_landmarks_paper = scaler.fit_transform(left_hand_landmarks_paper)\n",
    "            right_hand_landmarks_paper = scaler.fit_transform(right_hand_landmarks_paper)\n",
    "            left_hand_landmarks_rock = scaler.fit_transform(left_hand_landmarks_rock)\n",
    "            right_hand_landmarks_rock = scaler.fit_transform(right_hand_landmarks_rock)\n",
    "            left_hand_landmarks_etc = scaler.fit_transform(left_hand_landmarks_etc)\n",
    "            right_hand_landmarks_etc = scaler.fit_transform(right_hand_landmarks_etc)\n",
    "\n",
    "            data_left_hand_paper.append(left_hand_landmarks_paper)\n",
    "            data_right_hand_paper.append(right_hand_landmarks_paper)\n",
    "            data_left_hand_rock.append(left_hand_landmarks_rock)\n",
    "            data_right_hand_rock.append(right_hand_landmarks_rock)\n",
    "            data_left_hand_etc.append(left_hand_landmarks_etc)\n",
    "            data_right_hand_etc.append(right_hand_landmarks_etc)\n",
    "\n",
    "    return (data_left_hand_paper, data_right_hand_paper,\n",
    "            data_left_hand_rock, data_right_hand_rock,\n",
    "            data_left_hand_etc, data_right_hand_etc)\n",
    "\n",
    "# 데이터 특성 추출 및 전처리\n",
    "(data_left_hand_paper, data_right_hand_paper,\n",
    " data_left_hand_rock, data_right_hand_rock,\n",
    " data_left_hand_etc, data_right_hand_etc) = load_and_combine_data()\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_left_paper = np.array(data_left_hand_paper)\n",
    "X_right_paper = np.array(data_right_hand_paper)\n",
    "X_left_rock = np.array(data_left_hand_rock)\n",
    "X_right_rock = np.array(data_right_hand_rock)\n",
    "X_left_etc = np.array(data_left_hand_etc)\n",
    "X_right_etc = np.array(data_right_hand_etc)\n",
    "\n",
    "# 결합된 데이터 생성\n",
    "X_left = np.vstack((X_left_paper, X_left_rock, X_left_etc))\n",
    "X_right = np.vstack((X_right_paper, X_right_rock, X_right_etc))\n",
    "y = np.array([0] * len(X_left_paper) + [1] * len(X_left_rock) + [2] * len(X_left_etc))\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_left_train, X_left_test, X_right_train, X_right_test, y_train, y_test = train_test_split(\n",
    "    X_left, X_right, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# DNN 모델 생성\n",
    "inputs_left = tf.keras.Input(shape=(21, 2)) # shape는 데이터 개수를 의미하는 맨 앞의 축을 제외하고 입력\n",
    "inputs_right = tf.keras.Input(shape=(21, 2))\n",
    "x_left = layers.Flatten()(inputs_left)\n",
    "x_right = layers.Flatten()(inputs_right)\n",
    "x_left = layers.Dense(64, activation=\"relu\")(x_left)\n",
    "x_right = layers.Dense(64, activation=\"relu\")(x_right)\n",
    "x_left = layers.Dense(32, activation=\"relu\")(x_left)\n",
    "x_right = layers.Dense(32, activation=\"relu\")(x_right)\n",
    "x_left = layers.Dense(16, activation=\"relu\")(x_left)\n",
    "x_right = layers.Dense(16, activation=\"relu\")(x_right)\n",
    "x_left = layers.Dense(4)(x_left)\n",
    "x_right = layers.Dense(4)(x_right)\n",
    "out_left = layers.Softmax()(x_left)\n",
    "out_right = layers.Softmax()(x_right)\n",
    "\n",
    "model_left = tf.keras.Model(inputs=inputs_left, outputs=out_left)\n",
    "model_right = tf.keras.Model(inputs=inputs_right, outputs=out_right)\n",
    "\n",
    "# 모델 컴파일\n",
    "model_left.compile(optimizer='adam',\n",
    "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "model_right.compile(optimizer='adam',\n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model_left.fit(X_left_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "model_right.fit(X_right_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# 모델 평가\n",
    "test_loss_left, test_acc_left = model_left.evaluate(X_left_test, y_test)\n",
    "test_loss_right, test_acc_right = model_right.evaluate(X_right_test, y_test)\n",
    "\n",
    "print(f'Left Hand Test accuracy: {test_acc_left}')\n",
    "print(f'Right Hand Test accuracy: {test_acc_right}')\n",
    "\n",
    "# 모델 요약\n",
    "model_left.summary()\n",
    "model_right.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3308e78d-25a8-4226-875a-1ec097531cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: left_hand_gesture_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: left_hand_gesture_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: right_hand_gesture_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: right_hand_gesture_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow SavedModel 형식으로 모델 저장\n",
    "model_left.save(\"left_hand_gesture_model\", save_format='tf')\n",
    "model_right.save(\"right_hand_gesture_model\", save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d4464-c766-446c-a099-8e28d0dc6ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
